{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDP NF Code\n",
    "\n",
    "This is the unstructured code of IDP NF project.\n",
    "\n",
    "Each protein molecular is represented as $(\\theta)$ with dihedral angle $(i, j, k, l)$ which can be transformed into a standard 3D coordinate system $(x_1, x_2, x_3)$.\n",
    "\n",
    "The main structure is a normalizing flow consisting of $K$ $f_k$, which looks like [base] - {[circular shift]-[coupling layer]}$_{k=1}^K$ - [loss (energy + log absolute determinant)].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circular Shift\n",
    "\n",
    "This learned circular shift layer is to ensure that all angle coordinates are within $[0, 2\\pi]$.\n",
    "\n",
    "The formal formula is given by\n",
    "\n",
    "$$\\theta_{ni}\\rightarrow(\\theta_{ni}+c_i)\\text{ mod }2\\pi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Sequence, Callable, Mapping, Any, Tuple\n",
    "import functools\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from nflows.transforms.base import (\n",
    "    Transform, CompositeTransform\n",
    ")\n",
    "from nflows.transforms.linear import NaiveLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define class `CircularShift`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircularShift(Transform):\n",
    "    def __init__(self,\n",
    "                 shift: Tensor,\n",
    "                 lower: Union[float, Tensor],\n",
    "                 upper: Union[float, Tensor]):\n",
    "        if (not torch.is_tensor(lower)) and (not torch.is_tensor(upper)) and (lower >= upper):\n",
    "            raise ValueError('`lower` must be less than `upper`.')\n",
    "\n",
    "        try:\n",
    "            width = upper - lower\n",
    "        except TypeError as e:\n",
    "            raise ValueError('`lower` and `upper` must be broadcastable to same '\n",
    "                            f'shape, but `lower`={lower} and `upper`={upper}') from e\n",
    "\n",
    "        self.wrap = lambda x: torch.remainder(x - lower, width) + lower\n",
    "        self.shift = self.wrap(shift)\n",
    "    \n",
    "    def forward(self, inputs, context=None):\n",
    "        outputs = self.wrap(inputs + self.shift)\n",
    "        logabsdet = torch.zeros_like(inputs)\n",
    "        return outputs, logabsdet\n",
    "\n",
    "    def inverse(self, inputs, context=None):\n",
    "        outputs = self.wrap(inputs - self.shift)\n",
    "        logabsdet = torch.zeros_like(inputs)\n",
    "        return outputs, logabsdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coupling Flows\n",
    "## Step 1: Rewrite embeddings.circular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular(x: Tensor,\n",
    "             lower: float,\n",
    "             upper: float,\n",
    "             num_frequencies: int) -> Tensor:\n",
    "    \"\"\"Maps angles to points on the unit circle.\n",
    "\n",
    "    The mapping is such that the interval [lower, upper] is mapped to a full\n",
    "    circle starting and ending at (1, 0). For num_frequencies > 1, the mapping\n",
    "    also includes higher frequencies which are multiples of 2 pi/(lower-upper)\n",
    "    so that [lower, upper] wraps around the unit circle multiple times.\n",
    "\n",
    "    Args:\n",
    "        x: array of shape [..., D].\n",
    "        lower: lower limit, angles equal to this will be mapped to (1, 0).\n",
    "        upper: upper limit, angles equal to this will be mapped to (1, 0).\n",
    "        num_frequencies: number of frequencies to consider in the embedding.\n",
    "\n",
    "    Returns:\n",
    "        An array of shape [..., 2*num_frequencies*D].\n",
    "    \"\"\"\n",
    "    base_frequency = 2. * torch.pi / (upper - lower)\n",
    "    frequencies = base_frequency * torch.arange(1, num_frequencies+1)\n",
    "    angles = frequencies * (x[..., None] - lower)\n",
    "    # Reshape from [..., D, num_frequencies] to [..., D*num_frequencies].\n",
    "    angles = angles.reshape(x.shape[:-1] + (-1,))\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "    return torch.concat([cos, sin], axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Rewrite conditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape_last(x: Tensor, ndims: int, new_shape: Sequence[int]) -> Tensor:\n",
    "    \"\"\"Reshapes the last `ndims` dimensions of `x` to shape `new_shape`.\"\"\"\n",
    "    if ndims <= 0:\n",
    "        raise ValueError(\n",
    "            f'Number of dimensions to reshape must be positive, got {ndims}.')\n",
    "    return torch.reshape(x, x.shape[:-ndims] + tuple(new_shape))\n",
    "\n",
    "\n",
    "def make_equivariant_conditioner(\n",
    "    num_bijector_params: int,\n",
    "    lower: float,\n",
    "    upper: float,\n",
    "    embedding_size: int,\n",
    "    conditioner_constructor: Callable[..., Any],\n",
    "    conditioner_kwargs: Mapping[str, Any],\n",
    "    num_frequencies: int,\n",
    ") -> CompositeTransform:\n",
    "    \"\"\"Make a permutation-equivariant conditioner for the coupling flow.\"\"\"\n",
    "    # This conditioner assumes that the input is of shape [..., N, D1]. It returns\n",
    "    # an output of shape [..., N, D2, K], where:\n",
    "    #   D2 = `shape_transformed[-1]`\n",
    "    #   K = `num_bijector_params`\n",
    "    conditioner = conditioner_constructor(**conditioner_kwargs)\n",
    "    return CompositeTransform([\n",
    "        functools.partial(\n",
    "            circular, lower=lower, upper=upper,\n",
    "            num_frequencies=num_frequencies),\n",
    "        NaiveLinear(embedding_size),\n",
    "        conditioner,\n",
    "        NaiveLinear(num_bijector_params),\n",
    "        functools.partial(\n",
    "            _reshape_last, ndims=1, new_shape=(num_bijector_params)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Rewrite \n",
    "- distrax.SplitCoupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitCoupling(Transform):\n",
    "  \"\"\"Split coupling bijector, with arbitrary conditioner & inner bijector.\n",
    "  This coupling bijector splits the input array into two parts along a specified\n",
    "  axis. One part remains unchanged, whereas the other part is transformed by an\n",
    "  inner bijector conditioned on the unchanged part.\n",
    "  Let `f` be a conditional bijector (the inner bijector) and `g` be a function\n",
    "  (the conditioner). For `swap=False`, the split coupling bijector is defined as\n",
    "  follows:\n",
    "  - Forward:\n",
    "    ```\n",
    "    x = [x1, x2]\n",
    "    y1 = x1\n",
    "    y2 = f(x2; g(x1))\n",
    "    y = [y1, y2]\n",
    "    ```\n",
    "  - Forward Jacobian log determinant:\n",
    "    ```\n",
    "    x = [x1, x2]\n",
    "    log|det J(x)| = log|det df/dx2(x2; g(x1))|\n",
    "    ```\n",
    "  - Inverse:\n",
    "    ```\n",
    "    y = [y1, y2]\n",
    "    x1 = y1\n",
    "    x2 = f^{-1}(y2; g(y1))\n",
    "    x = [x1, x2]\n",
    "    ```\n",
    "  - Inverse Jacobian log determinant:\n",
    "    ```\n",
    "    y = [y1, y2]\n",
    "    log|det J(y)| = log|det df^{-1}/dy2(y2; g(y1))|\n",
    "    ```\n",
    "  Here, `[x1, x2]` is a partition of `x` along some axis. By default, `x1`\n",
    "  remains unchanged and `x2` is transformed. If `swap=True`, `x2` will remain\n",
    "  unchanged and `x1` will be transformed.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               angles: Tensor,\n",
    "               conditioner: function,\n",
    "               bijector: function,):\n",
    "    \"\"\"Initializes a SplitCoupling bijector.\n",
    "    Args:\n",
    "      conditioner: a function that computes the parameters of the inner bijector\n",
    "        as a function of the unchanged part of the input. The output of the\n",
    "        conditioner will be passed to `bijector` in order to obtain the inner\n",
    "        bijector.\n",
    "      bijector: a callable that returns the inner bijector that will be used to\n",
    "        transform one of the two parts. The input to `bijector` is a set of\n",
    "        parameters that can be used to configure the inner bijector. The\n",
    "        `event_ndims_in` and `event_ndims_out` of the inner bijector must be\n",
    "        equal, and less than or equal to `event_ndims`. If they are less than\n",
    "        `event_ndims`, the remaining dimensions will be converted to event\n",
    "        dimensions using `distrax.Block`.\n",
    "    \"\"\"\n",
    "    self._angles = angles\n",
    "    self._conditioner = conditioner\n",
    "    self._bijector = bijector\n",
    "\n",
    "  @property\n",
    "  def bijector(self) -> function:\n",
    "    \"\"\"The callable that returns the inner bijector of `SplitCoupling`.\"\"\"\n",
    "    return self._bijector\n",
    "\n",
    "  @property\n",
    "  def conditioner(self) -> function:\n",
    "    \"\"\"The conditioner function.\"\"\"\n",
    "    return self._conditioner\n",
    "\n",
    "  def forward(self, x: Tensor, context=None) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Computes y = f(x) and log|det J(f)(x)|.\"\"\"\n",
    "    params = self._conditioner(self._angles)\n",
    "    y, logdet = self._bijector(params).forward(x)\n",
    "    return y, logdet\n",
    "\n",
    "  def inverse(self, y: Tensor, context=None) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Computes x = f^{-1}(y) and log|det J(f^{-1})(y)|.\"\"\"\n",
    "    params = self._conditioner(self._angles)\n",
    "    x, logdet = self._bijector(params).inverse(y)\n",
    "    return x, logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- distrax.RationalQuadraticSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\"\"\"Rational-quadratic spline bijector.\"\"\"\n",
    "def _normalize_bin_sizes(unnormalized_bin_sizes: Tensor,\n",
    "                         total_size: float,\n",
    "                         min_bin_size: float) -> Tensor:\n",
    "  \"\"\"Make bin sizes sum to `total_size` and be no less than `min_bin_size`.\"\"\"\n",
    "  num_bins = unnormalized_bin_sizes.shape[-1]\n",
    "  if num_bins * min_bin_size > total_size:\n",
    "    raise ValueError(\n",
    "        f'The number of bins ({num_bins}) times the minimum bin size'\n",
    "        f' ({min_bin_size}) cannot be greater than the total bin size'\n",
    "        f' ({total_size}).')\n",
    "  bin_sizes = torch.softmax(unnormalized_bin_sizes, axis=-1)\n",
    "  return bin_sizes * (total_size - num_bins * min_bin_size) + min_bin_size\n",
    "\n",
    "\n",
    "def _normalize_knot_slopes(unnormalized_knot_slopes: Tensor,\n",
    "                           min_knot_slope: float) -> Tensor:\n",
    "  \"\"\"Make knot slopes be no less than `min_knot_slope`.\"\"\"\n",
    "  # The offset is such that the normalized knot slope will be equal to 1\n",
    "  # whenever the unnormalized knot slope is equal to 0.\n",
    "  if min_knot_slope >= 1.:\n",
    "    raise ValueError(f'The minimum knot slope must be less than 1; got'\n",
    "                     f' {min_knot_slope}.')\n",
    "  min_knot_slope = torch.tensor(\n",
    "      min_knot_slope, dtype=unnormalized_knot_slopes.dtype)\n",
    "  offset = torch.log(torch.exp(1. - min_knot_slope) - 1.)\n",
    "  softplus = torch.nn.Softplus()\n",
    "  return softplus(unnormalized_knot_slopes + offset) + min_knot_slope\n",
    "\n",
    "\n",
    "def _rational_quadratic_spline_fwd(x: Tensor,\n",
    "                                   x_pos: Tensor,\n",
    "                                   y_pos: Tensor,\n",
    "                                   knot_slopes: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "  \"\"\"Applies a rational-quadratic spline to a scalar.\n",
    "  Args:\n",
    "    x: a scalar (0-dimensional array). The scalar `x` can be any real number; it\n",
    "      will be transformed by the spline if it's in the closed interval\n",
    "      `[x_pos[0], x_pos[-1]]`, and it will be transformed linearly if it's\n",
    "      outside that interval.\n",
    "    x_pos: array of shape [num_bins + 1], the bin boundaries on the x axis.\n",
    "    y_pos: array of shape [num_bins + 1], the bin boundaries on the y axis.\n",
    "    knot_slopes: array of shape [num_bins + 1], the slopes at the knot points.\n",
    "  Returns:\n",
    "    A tuple of two scalars: the output of the transformation and the log of the\n",
    "    absolute first derivative at `x`.\n",
    "  \"\"\"\n",
    "  # Search to find the right bin. NOTE: The bins are sorted, so we could use\n",
    "  # binary search, but this is more GPU/TPU friendly.\n",
    "  # The following implementation avoids indexing for faster TPU computation.\n",
    "  below_range = x <= x_pos[0]\n",
    "  above_range = x >= x_pos[-1]\n",
    "  correct_bin = torch.logical_and(x[..., None] >= x_pos[:-1][None, None, ...], \n",
    "                                  x[..., None] < x_pos[1:][None, None, ...])\n",
    "  any_bin_in_range = torch.any(correct_bin, dim=2)\n",
    "  first_bin = torch.concat([torch.tensor([1], dtype=bool),\n",
    "                            torch.zeros(correct_bin.shape[-1]-1, dtype=bool)])\n",
    "  # If y does not fall into any bin, we use the first spline in the following\n",
    "  # computations to avoid numerical issues.\n",
    "  correct_bin[~any_bin_in_range] = first_bin\n",
    "  # Dot product of each parameter with the correct bin mask.\n",
    "  params = torch.stack([x_pos, y_pos, knot_slopes], axis=1)\n",
    "  \n",
    "  params_bin_left = torch.sum(correct_bin[..., None] * params[:-1], axis=2)\n",
    "  params_bin_right = torch.sum(correct_bin[..., None] * params[1:], axis=2)\n",
    "\n",
    "  x_pos_bin = (params_bin_left[..., 0], params_bin_right[..., 0])\n",
    "  y_pos_bin = (params_bin_left[..., 1], params_bin_right[..., 1])\n",
    "  knot_slopes_bin = (params_bin_left[..., 2], params_bin_right[..., 2])\n",
    "\n",
    "  bin_width = x_pos_bin[1] - x_pos_bin[0]\n",
    "  bin_height = y_pos_bin[1] - y_pos_bin[0]\n",
    "  bin_slope = bin_height / bin_width\n",
    "\n",
    "  z = (x - x_pos_bin[0]) / bin_width\n",
    "  # `z` should be in range [0, 1] to avoid NaNs later. This can happen because\n",
    "  # of small floating point issues or when x is outside of the range of bins.\n",
    "  # To avoid all problems, we restrict z in [0, 1].\n",
    "  z = torch.clip(z, 0., 1.)\n",
    "  sq_z = z * z\n",
    "  z1mz = z - sq_z  # z(1-z)\n",
    "  sq_1mz = (1. - z) ** 2\n",
    "  slopes_term = knot_slopes_bin[1] + knot_slopes_bin[0] - 2. * bin_slope\n",
    "  numerator = bin_height * (bin_slope * sq_z + knot_slopes_bin[0] * z1mz)\n",
    "  denominator = bin_slope + slopes_term * z1mz\n",
    "  y = y_pos_bin[0] + numerator / denominator\n",
    "\n",
    "  # Compute log det Jacobian.\n",
    "  # The logdet is a sum of 3 logs. It is easy to see that the inputs of the\n",
    "  # first two logs are guaranteed to be positive because we ensured that z is in\n",
    "  # [0, 1]. This is also true of the log(denominator) because:\n",
    "  # denominator\n",
    "  # == bin_slope + (knot_slopes_bin[1] + knot_slopes_bin[0] - 2 * bin_slope) *\n",
    "  # z*(1-z)\n",
    "  # >= bin_slope - 2 * bin_slope * z * (1-z)\n",
    "  # >= bin_slope - 2 * bin_slope * (1/4)\n",
    "  # == bin_slope / 2\n",
    "  logdet = 2. * torch.log(bin_slope) + torch.log(\n",
    "      knot_slopes_bin[1] * sq_z + 2. * bin_slope * z1mz +\n",
    "      knot_slopes_bin[0] * sq_1mz) - 2. * torch.log(denominator)\n",
    "\n",
    "  # If x is outside the spline range, we default to a linear transformation.\n",
    "  y = torch.where(below_range, (x - x_pos[0]) * knot_slopes[0] + y_pos[0], y)\n",
    "  y = torch.where(above_range, (x - x_pos[-1]) * knot_slopes[-1] + y_pos[-1], y)\n",
    "  logdet = torch.where(below_range, torch.log(knot_slopes[0]), logdet)\n",
    "  logdet = torch.where(above_range, torch.log(knot_slopes[-1]), logdet)\n",
    "  return y, logdet\n",
    "\n",
    "\n",
    "def _safe_quadratic_root(a: Tensor, b: Tensor, c: Tensor) -> Tensor:\n",
    "  \"\"\"Implement a numerically stable version of the quadratic formula.\"\"\"\n",
    "  # This is not a general solution to the quadratic equation, as it assumes\n",
    "  # b ** 2 - 4. * a * c is known a priori to be positive (and which of the two\n",
    "  # roots is to be used, see https://arxiv.org/abs/1906.04032).\n",
    "  # There are two sources of instability:\n",
    "  # (a) When b ** 2 - 4. * a * c -> 0, sqrt gives NaNs in gradient.\n",
    "  # We clip sqrt_diff to have the smallest float number.\n",
    "  sqrt_diff = b ** 2 - 4. * a * c\n",
    "  safe_sqrt = torch.sqrt(torch.clip(sqrt_diff, torch.finfo(sqrt_diff.dtype).tiny))\n",
    "  # If sqrt_diff is non-positive, we set sqrt to 0. as it should be positive.\n",
    "  safe_sqrt = torch.where(sqrt_diff > 0., safe_sqrt, 0.)\n",
    "  # (b) When 4. * a * c -> 0. We use the more stable quadratic solution\n",
    "  # depending on the sign of b.\n",
    "  # See https://people.csail.mit.edu/bkph/articles/Quadratics.pdf (eq 7 and 8).\n",
    "  # Solution when b >= 0\n",
    "  numerator_1 = 2. * c\n",
    "  denominator_1 = -b - safe_sqrt\n",
    "  # Solution when b < 0\n",
    "  numerator_2 = - b + safe_sqrt\n",
    "  denominator_2 = 2 * a\n",
    "  # Choose the numerically stable solution.\n",
    "  numerator = torch.where(b >= 0, numerator_1, numerator_2)\n",
    "  denominator = torch.where(b >= 0, denominator_1, denominator_2)\n",
    "  return numerator / denominator\n",
    "\n",
    "\n",
    "def _rational_quadratic_spline_inv(y: Tensor,\n",
    "                                   x_pos: Tensor,\n",
    "                                   y_pos: Tensor,\n",
    "                                   knot_slopes: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "  \"\"\"Applies the inverse of a rational-quadratic spline to a scalar.\n",
    "  Args:\n",
    "    y: a scalar (0-dimensional array). The scalar `y` can be any real number; it\n",
    "      will be transformed by the spline if it's in the closed interval\n",
    "      `[y_pos[0], y_pos[-1]]`, and it will be transformed linearly if it's\n",
    "      outside that interval.\n",
    "    x_pos: array of shape [num_bins + 1], the bin boundaries on the x axis.\n",
    "    y_pos: array of shape [num_bins + 1], the bin boundaries on the y axis.\n",
    "    knot_slopes: array of shape [num_bins + 1], the slopes at the knot points.\n",
    "  Returns:\n",
    "    A tuple of two scalars: the output of the inverse transformation and the log\n",
    "    of the absolute first derivative of the inverse at `y`.\n",
    "  \"\"\"\n",
    "  # Search to find the right bin. NOTE: The bins are sorted, so we could use\n",
    "  # binary search, but this is more GPU/TPU friendly.\n",
    "  # The following implementation avoids indexing for faster TPU computation.\n",
    "  below_range = y <= y_pos[0]\n",
    "  above_range = y >= y_pos[-1]\n",
    "  correct_bin = torch.logical_and(y[..., None] >= y_pos[:-1][None, None, ...], \n",
    "                                  y[..., None] < y_pos[1:][None, None, ...])\n",
    "  any_bin_in_range = torch.any(correct_bin)\n",
    "  first_bin = torch.concat([torch.tensor([1], dtype=bool),\n",
    "                            torch.zeros(correct_bin.shape[-1]-1, dtype=bool)])\n",
    "  # If y does not fall into any bin, we use the first spline in the following\n",
    "  # computations to avoid numerical issues.\n",
    "  correct_bin[~any_bin_in_range] = first_bin\n",
    "  # Dot product of each parameter with the correct bin mask.\n",
    "  params = torch.stack([x_pos, y_pos, knot_slopes], axis=1)\n",
    "  params_bin_left = torch.sum(correct_bin[..., None] * params[:-1], axis=2)\n",
    "  params_bin_right = torch.sum(correct_bin[..., None] * params[1:], axis=2)\n",
    "\n",
    "  # These are the parameters for the corresponding bin.\n",
    "  x_pos_bin = (params_bin_left[..., 0], params_bin_right[..., 0])\n",
    "  y_pos_bin = (params_bin_left[..., 1], params_bin_right[..., 1])\n",
    "  knot_slopes_bin = (params_bin_left[..., 2], params_bin_right[..., 2])\n",
    "\n",
    "  bin_width = x_pos_bin[1] - x_pos_bin[0]\n",
    "  bin_height = y_pos_bin[1] - y_pos_bin[0]\n",
    "  bin_slope = bin_height / bin_width\n",
    "  w = (y - y_pos_bin[0]) / bin_height\n",
    "  w = torch.clip(w, 0., 1.)  # Ensure w is in [0, 1].\n",
    "  # Compute quadratic coefficients: az^2 + bz + c = 0\n",
    "  slopes_term = knot_slopes_bin[1] + knot_slopes_bin[0] - 2. * bin_slope\n",
    "  c = - bin_slope * w\n",
    "  b = knot_slopes_bin[0] - slopes_term * w\n",
    "  a = bin_slope - b\n",
    "\n",
    "  # Solve quadratic to obtain z and then x.\n",
    "  z = _safe_quadratic_root(a, b, c)\n",
    "  z = torch.clip(z, 0., 1.)  # Ensure z is in [0, 1].\n",
    "  x = bin_width * z + x_pos_bin[0]\n",
    "\n",
    "  # Compute log det Jacobian.\n",
    "  sq_z = z * z\n",
    "  z1mz = z - sq_z  # z(1-z)\n",
    "  sq_1mz = (1. - z) ** 2\n",
    "  denominator = bin_slope + slopes_term * z1mz\n",
    "  logdet = - 2. * torch.log(bin_slope) - torch.log(\n",
    "      knot_slopes_bin[1] * sq_z + 2. * bin_slope * z1mz +\n",
    "      knot_slopes_bin[0] * sq_1mz) + 2. * torch.log(denominator)\n",
    "\n",
    "  # If y is outside the spline range, we default to a linear transformation.\n",
    "  x = torch.where(below_range, (y - y_pos[0]) / knot_slopes[0] + x_pos[0], x)\n",
    "  x = torch.where(above_range, (y - y_pos[-1]) / knot_slopes[-1] + x_pos[-1], x)\n",
    "  logdet = torch.where(below_range, - torch.log(knot_slopes[0]), logdet)\n",
    "  logdet = torch.where(above_range, - torch.log(knot_slopes[-1]), logdet)\n",
    "  return x, logdet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RationalQuadraticSpline(Transform):\n",
    "  \"\"\"A rational-quadratic spline bijector.\n",
    "  Implements the spline bijector introduced by:\n",
    "  > Durkan et al., Neural Spline Flows, https://arxiv.org/abs/1906.04032, 2019.\n",
    "  This bijector is a monotonically increasing spline operating on an interval\n",
    "  [a, b], such that f(a) = a and f(b) = b. Outside the interval [a, b], the\n",
    "  bijector defaults to a linear transformation whose slope matches that of the\n",
    "  spline at the nearest boundary (either a or b). The range boundaries a and b\n",
    "  are hyperparameters passed to the constructor.\n",
    "  The spline on the interval [a, b] consists of `num_bins` segments, on each of\n",
    "  which the spline takes the form of a rational quadratic (ratio of two\n",
    "  quadratic polynomials). The first derivative of the bijector is guaranteed to\n",
    "  be continuous on the whole real line. The second derivative is generally not\n",
    "  continuous at the knot points (bin boundaries).\n",
    "  The spline is parameterized by the bin sizes on the x and y axis, and by the\n",
    "  slopes at the knot points. All spline parameters are passed to the constructor\n",
    "  as an unconstrained array `params` of shape `[..., 3 * num_bins + 1]`. The\n",
    "  spline parameters are extracted from `params`, and are reparameterized\n",
    "  internally as appropriate. The number of bins is a hyperparameter, and is\n",
    "  implicitly defined by the last dimension of `params`.\n",
    "  This bijector is applied elementwise. Given some input `x`, the parameters\n",
    "  `params` and the input `x` are broadcast against each other. For example,\n",
    "  suppose `x` is of shape `[N, D]`. Then:\n",
    "  - If `params` is of shape `[3 * num_bins + 1]`, the same spline is identically\n",
    "    applied to each element of `x`.\n",
    "  - If `params` is of shape `[D, 3 * num_bins + 1]`, the same spline is applied\n",
    "    along the first axis of `x` but a different spline is applied along the\n",
    "    second axis of `x`.\n",
    "  - If `params` is of shape `[N, D, 3 * num_bins + 1]`, a different spline is\n",
    "    applied to each element of `x`.\n",
    "  - If `params` is of shape `[M, N, D, 3 * num_bins + 1]`, `M` different splines\n",
    "    are applied to each element of `x`, and the output is of shape `[M, N, D]`.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               params: Tensor,\n",
    "               range_min: float,\n",
    "               range_max: float,\n",
    "               boundary_slopes: str = 'unconstrained',\n",
    "               min_bin_size: float = 1e-4,\n",
    "               min_knot_slope: float = 1e-4):\n",
    "    \"\"\"Initializes a RationalQuadraticSpline bijector.\n",
    "    Args:\n",
    "      params: array of shape `[..., 3 * num_bins + 1]`, the unconstrained\n",
    "        parameters of the bijector. The number of bins is implicitly defined by\n",
    "        the last dimension of `params`. The parameters can take arbitrary\n",
    "        unconstrained values; the bijector will reparameterize them internally\n",
    "        and make sure they obey appropriate constraints. If `params` is the\n",
    "        all-zeros array, the bijector becomes the identity function everywhere\n",
    "        on the real line.\n",
    "      range_min: the lower bound of the spline's range. Below `range_min`, the\n",
    "        bijector defaults to a linear transformation.\n",
    "      range_max: the upper bound of the spline's range. Above `range_max`, the\n",
    "        bijector defaults to a linear transformation.\n",
    "      boundary_slopes: controls the behaviour of the slope of the spline at the\n",
    "        range boundaries (`range_min` and `range_max`). It is used to enforce\n",
    "        certain boundary conditions on the spline. Available options are:\n",
    "        - 'unconstrained': no boundary conditions are imposed; the slopes at the\n",
    "          boundaries can vary freely.\n",
    "        - 'lower_identity': the slope of the spline is set equal to 1 at the\n",
    "          lower boundary (`range_min`). This makes the bijector equal to the\n",
    "          identity function for values less than `range_min`.\n",
    "        - 'upper_identity': similar to `lower_identity`, but now the slope of\n",
    "          the spline is set equal to 1 at the upper boundary (`range_max`). This\n",
    "          makes the bijector equal to the identity function for values greater\n",
    "          than `range_max`.\n",
    "        - 'identity': combines the effects of 'lower_identity' and\n",
    "          'upper_identity' together. The slope of the spline is set equal to 1\n",
    "          at both boundaries (`range_min` and `range_max`). This makes the\n",
    "          bijector equal to the identity function outside the interval\n",
    "          `[range_min, range_max]`.\n",
    "        - 'circular': makes the slope at `range_min` and `range_max` be the\n",
    "          same. This implements the \"circular spline\" introduced by:\n",
    "          > Rezende et al., Normalizing Flows on Tori and Spheres,\n",
    "          > https://arxiv.org/abs/2002.02428, 2020.\n",
    "          This option should be used when the spline operates on a circle\n",
    "          parameterized by an angle in the interval `[range_min, range_max]`,\n",
    "          where `range_min` and `range_max` correspond to the same point on the\n",
    "          circle.\n",
    "      min_bin_size: The minimum bin size, in either the x or the y axis. Should\n",
    "        be a small positive number, chosen for numerical stability. Guarantees\n",
    "        that no bin in either the x or the y axis will be less than this value.\n",
    "      min_knot_slope: The minimum slope at each knot point. Should be a small\n",
    "        positive number, chosen for numerical stability. Guarantess that no knot\n",
    "        will have a slope less than this value.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    if params.shape[-1] % 3 != 1 or params.shape[-1] < 4:\n",
    "      raise ValueError(f'The last dimension of `params` must have size'\n",
    "                       f' `3 * num_bins + 1` and `num_bins` must be at least 1.'\n",
    "                       f' Got size {params.shape[-1]}.')\n",
    "    if range_min >= range_max:\n",
    "      raise ValueError(f'`range_min` must be less than `range_max`. Got'\n",
    "                       f' `range_min={range_min}` and `range_max={range_max}`.')\n",
    "    if min_bin_size <= 0.:\n",
    "      raise ValueError(f'The minimum bin size must be positive; got'\n",
    "                       f' {min_bin_size}.')\n",
    "    if min_knot_slope <= 0.:\n",
    "      raise ValueError(f'The minimum knot slope must be positive; got'\n",
    "                       f' {min_knot_slope}.')\n",
    "    self._dtype = params.dtype\n",
    "    self._num_bins = (params.shape[-1] - 1) // 3\n",
    "    # Extract unnormalized parameters.\n",
    "    unnormalized_bin_widths = params[..., :self._num_bins]\n",
    "    unnormalized_bin_heights = params[..., self._num_bins : 2 * self._num_bins]\n",
    "    unnormalized_knot_slopes = params[..., 2 * self._num_bins:]\n",
    "    # Normalize bin sizes and compute bin positions on the x and y axis.\n",
    "    range_size = range_max - range_min\n",
    "    bin_widths = _normalize_bin_sizes(unnormalized_bin_widths, range_size,\n",
    "                                      min_bin_size)\n",
    "    bin_heights = _normalize_bin_sizes(unnormalized_bin_heights, range_size,\n",
    "                                       min_bin_size)\n",
    "    x_pos = range_min + torch.cumsum(bin_widths[..., :-1], axis=-1)\n",
    "    y_pos = range_min + torch.cumsum(bin_heights[..., :-1], axis=-1)\n",
    "    pad_shape = params.shape[:-1] + (1,)\n",
    "    pad_below = torch.full(pad_shape, range_min, dtype=self._dtype)\n",
    "    pad_above = torch.full(pad_shape, range_max, dtype=self._dtype)\n",
    "    self._x_pos = torch.concat([pad_below, x_pos, pad_above], axis=-1)\n",
    "    self._y_pos = torch.concat([pad_below, y_pos, pad_above], axis=-1)\n",
    "    # Normalize knot slopes and enforce requested boundary conditions.\n",
    "    knot_slopes = _normalize_knot_slopes(unnormalized_knot_slopes,\n",
    "                                         min_knot_slope)\n",
    "    if boundary_slopes == 'unconstrained':\n",
    "      self._knot_slopes = knot_slopes\n",
    "    elif boundary_slopes == 'lower_identity':\n",
    "      ones = torch.ones(pad_shape, self._dtype)\n",
    "      self._knot_slopes = torch.concat([ones, knot_slopes[..., 1:]], axis=-1)\n",
    "    elif boundary_slopes == 'upper_identity':\n",
    "      ones = torch.ones(pad_shape, self._dtype)\n",
    "      self._knot_slopes = torch.concat(\n",
    "          [knot_slopes[..., :-1], ones], axis=-1)\n",
    "    elif boundary_slopes == 'identity':\n",
    "      ones = torch.ones(pad_shape, self._dtype)\n",
    "      self._knot_slopes = torch.concat(\n",
    "          [ones, knot_slopes[..., 1:-1], ones], axis=-1)\n",
    "    elif boundary_slopes == 'circular':\n",
    "      self._knot_slopes = torch.concat(\n",
    "          [knot_slopes[..., :-1], knot_slopes[..., :1]], axis=-1)\n",
    "    else:\n",
    "      raise ValueError(f'Unknown option for boundary slopes:'\n",
    "                       f' `{boundary_slopes}`.')\n",
    "\n",
    "  @property\n",
    "  def num_bins(self) -> int:\n",
    "    \"\"\"The number of segments on the interval.\"\"\"\n",
    "    return self._num_bins\n",
    "\n",
    "  @property\n",
    "  def knot_slopes(self) -> Tensor:\n",
    "    \"\"\"The slopes at the knot points.\"\"\"\n",
    "    return self._knot_slopes\n",
    "\n",
    "  @property\n",
    "  def x_pos(self) -> Tensor:\n",
    "    \"\"\"The bin boundaries on the `x`-axis.\"\"\"\n",
    "    return self._x_pos\n",
    "\n",
    "  @property\n",
    "  def y_pos(self) -> Tensor:\n",
    "    \"\"\"The bin boundaries on the `y`-axis.\"\"\"\n",
    "    return self._y_pos\n",
    "\n",
    "  def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Computes y = f(x) and log|det J(f)(x)|.\"\"\"\n",
    "    y, logdet = _rational_quadratic_spline_fwd(\n",
    "      x, self._x_pos, self._y_pos, self._knot_slopes)\n",
    "    return y, logdet\n",
    "\n",
    "  def inverse(self, y: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"Computes x = f^{-1}(y) and log|det J(f^{-1})(y)|.\"\"\"\n",
    "    x, logdet = _rational_quadratic_spline_inv(\n",
    "        y, self._x_pos, self._y_pos, self._knot_slopes)\n",
    "    return x, logdet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Rewrite coupling layers as follows\n",
    "[$x_1$] - [conditioner ($x_2$) ($C$)] - [rational quadratic spline ($G$ paramed by $C$)]\n",
    "\n",
    "[$x_2$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_coupling_flow(\n",
    "    angles: Tensor,\n",
    "    lower: float,\n",
    "    upper: float,\n",
    "    num_layers: int,\n",
    "    num_bins: int,\n",
    "    conditioner: Mapping[str, Any],\n",
    "    use_circular_shift: bool,\n",
    "    circular_shift_init: function = torch.zeros,\n",
    ") -> CompositeTransform:\n",
    "  \"\"\"Create a flow that consists of a sequence of split coupling layers.\n",
    "\n",
    "  All coupling layers use rational-quadratic splines. Each layer of the flow\n",
    "  is composed of two split coupling bijectors, where each coupling bijector\n",
    "  transforms a different part of the input.\n",
    "\n",
    "  The flow maps to and from the range `[lower, upper]`, obeying periodic\n",
    "  boundary conditions.\n",
    "\n",
    "  Args:\n",
    "    angles: a Tensor of shape (N) whose N is the batch size and 1 is angles.\n",
    "    lower: lower range of the flow.\n",
    "    upper: upper range of the flow.\n",
    "    num_layers: the number of layers to use. Each layer consists of two split\n",
    "      coupling bijectors, where each coupling bijector transforms a different\n",
    "      part of the input.\n",
    "    num_bins: number of bins to use in the rational-quadratic splines.\n",
    "    conditioner: a Mapping containing 'constructor' and 'kwargs' keys that\n",
    "      configures the conditioner used in the coupling layers.\n",
    "    use_circular_shift: if True, add a learned circular shift between successive\n",
    "      flow layers.\n",
    "    circular_shift_init: initializer for the circular shifts.\n",
    "\n",
    "  Returns:\n",
    "    The flow, a Distrax bijector.\n",
    "  \"\"\"\n",
    "\n",
    "  def bijector_fn(params: Tensor):\n",
    "    return RationalQuadraticSpline(\n",
    "        params,\n",
    "        range_min=lower,\n",
    "        range_max=upper,\n",
    "        boundary_slopes='circular',\n",
    "        min_bin_size=(upper - lower) * 1e-4)\n",
    "\n",
    "  layers = []\n",
    "  for _ in range(num_layers):\n",
    "    sublayers = []\n",
    "\n",
    "    # Circular shift.\n",
    "    if use_circular_shift:\n",
    "      shift = torch.nn.Parameter(\n",
    "          name='circular_shift',\n",
    "          param_name='shift',\n",
    "          shape=(1),\n",
    "          init=circular_shift_init)()\n",
    "      shift_layer = CircularShift(\n",
    "          (upper - lower) * shift, lower, upper)\n",
    "      sublayers.append(shift_layer)\n",
    "\n",
    "    # Coupling layer.\n",
    "    coupling_layer = SplitCoupling(\n",
    "        angles=angles,\n",
    "        bijector=bijector_fn,\n",
    "        conditioner=conditioner['constructor'](\n",
    "            num_bijector_params=3 * num_bins + 1,\n",
    "            lower=lower,\n",
    "            upper=upper,\n",
    "            **conditioner['kwargs'])\n",
    "        )\n",
    "    sublayers.append(coupling_layer)\n",
    "    layers.append(CompositeTransform(sublayers))\n",
    "\n",
    "  return CompositeTransform(layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Rewrite rdkit setDihedralRad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.rdchem import Mol\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from rdkit import Chem\n",
    "\n",
    "class Dihedral2Coord(Module):\n",
    "    \"\"\"Transform dihedral angles of a batch of conformers into 3D coordinates.\"\"\"\n",
    "\n",
    "    def __init__(self, mol: Mol, angles: Tensor):\n",
    "        \"\"\"\n",
    "        Initialization of D2C layer.\n",
    "\n",
    "        Args:\n",
    "            mol (Mol): N molecular conformation with the same backbone and possibly different dihedral angles.\n",
    "            angles (Tensor): a Tensor of shape (K, 4) where K is the number of dihedral angles for a conformer, 4 is (iAtomId, jAtomId, kAtomId, lAtomId).\n",
    "        \"\"\"\n",
    "        self.mol = mol\n",
    "        self.angles = angles\n",
    "        self.alist = {}\n",
    "        self.toBeMovedIdxList()\n",
    "\n",
    "\n",
    "    def toBeMovedIdxList(self):\n",
    "        \"\"\"\n",
    "        An implementation of toBeMovedIdxList from rdkit.\n",
    "        See https://github.com/rdkit/rdkit/blob/master/Code/GraphMol/MolTransforms/MolTransforms.cpp#L426\n",
    "        \"\"\"\n",
    "        nAtoms = self.mol.GetNumAtoms()\n",
    "        K = self.angles.shape[0]\n",
    "        for i in K:\n",
    "            iAtomId = self.angles[i, 1].item()\n",
    "            jAtomId = self.angles[i, 2].item()\n",
    "            if (iAtomId, jAtomId) not in self.alist:\n",
    "                self.alist[(iAtomId, jAtomId)] = []\n",
    "                visitedIdx = [False for _ in range(nAtoms)]\n",
    "                stack = []\n",
    "                stack.append(jAtomId)\n",
    "                visitedIdx[iAtomId] = 1\n",
    "                visitedIdx[jAtomId] = 1\n",
    "                tIdx = 0\n",
    "                wIdx = 0\n",
    "                doMainLoop = True\n",
    "                while len(stack) > 0:\n",
    "                    doMainLoop = False\n",
    "                    tIdx = stack[0]\n",
    "                    tAtom = self.mol.GetAtomWithIdx(tIdx)\n",
    "                    neighbors = tAtom.GetNeighbors()\n",
    "                    nbrIdx = 0\n",
    "                    endNbrs = len(neighbors)\n",
    "                    while nbrIdx != endNbrs:\n",
    "                        wIdx = neighbors[nbrIdx].GetIdx()\n",
    "                        if not visitedIdx[wIdx]:\n",
    "                            visitedIdx[wIdx] = 1\n",
    "                            stack.append(wIdx)\n",
    "                            doMainLoop = True\n",
    "                            break\n",
    "                        nbrIdx += 1\n",
    "                    if doMainLoop:\n",
    "                        continue\n",
    "                    visitedIdx[tIdx] = 1\n",
    "                    stack.pop()\n",
    "                self.alist[(iAtomId, jAtomId)].clear()\n",
    "                for j in range(nAtoms):\n",
    "                    if visitedIdx[j] and j != iAtomId:\n",
    "                        self.alist[(iAtomId, jAtomId)].append(j)\n",
    "\n",
    "\n",
    "    def transformPoint(self, pt: Tensor, angle: Tensor, axis: Tensor):\n",
    "        \"\"\"\n",
    "        An implementation of differentiable SetRotation and TransformPoint from rdkit.\n",
    "        See https://github.com/rdkit/rdkit/blob/master/Code/Geometry/Transform3D.cpp\n",
    "\n",
    "        Args:\n",
    "            pt (Tensor): a Tensor of shape (N, 3) where N is the batch size, 3 is 3D coordinates.\n",
    "            angle (Tensor): a Tensor of shape (N) where N is the batch size, 1 is the rotation angle.\n",
    "            axis (Tensor): a Tensor of shape (N, 3) where N is the batch size, 3 is 3D coordinates of the axis.\n",
    "        \"\"\"\n",
    "        N = pt.shape[0]\n",
    "        data = torch.eye(4).reshape(1, 4, 4).repeat(N, 1, 1)\n",
    "        cosT = angle.cos()\n",
    "        sinT = angle.sin()\n",
    "        t = 1 - cosT\n",
    "        X = axis[:, 0]\n",
    "        Y = axis[:, 1]\n",
    "        Z = axis[:, 2]\n",
    "        data[:, 0, 0] = t * X * X + cosT\n",
    "        data[:, 0, 1] = t * X * Y - sinT * Z\n",
    "        data[:, 0, 2] = t * X * Z + sinT * Y\n",
    "        data[:, 1, 0] = t * X * Y + sinT * Z\n",
    "        data[:, 1, 1] = t * Y * Y + cosT\n",
    "        data[:, 1, 2] = t * Y * Z - sinT * X\n",
    "        data[:, 2, 0] = t * X * Z - sinT * Y\n",
    "        data[:, 2, 1] = t * Y * Z + sinT * X\n",
    "        data[:, 2, 2] = t * Z * Z + cosT\n",
    "        x = data[:, 0, 0] * pt[:, 0] + data[:, 0, 1] * pt[:, 1] + data[:, 0, 2] * pt[:, 2] + data[:, 0, 3]\n",
    "        y = data[:, 1, 0] * pt[:, 0] + data[:, 1, 1] * pt[:, 1] + data[:, 1, 2] * pt[:, 2] + data[:, 1, 3]\n",
    "        z = data[:, 2, 0] * pt[:, 0] + data[:, 2, 1] * pt[:, 1] + data[:, 2, 2] * pt[:, 2] + data[:, 2, 3]\n",
    "        pt[:, 0] = x\n",
    "        pt[:, 1] = y\n",
    "        pt[:, 2] = z\n",
    "\n",
    "\n",
    "    def setDihedralRad(self, input: Tensor, angle: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        An implementation of differentiable setDihedralRad from rdkit.\n",
    "        Note: This version has eliminated all fault checks temporarily. Add them if needed from the link below.\n",
    "        See https://github.com/rdkit/rdkit/blob/master/Code/GraphMol/MolTransforms/MolTransforms.cpp#L612\n",
    "\n",
    "        Args:\n",
    "            mol (Mol): N molecular conformation with the same backbone and possibly different dihedral angles.\n",
    "            input (Tensor): a Tensor of shape (N) where N is the batch size, 1 is (dihedral angle value).\n",
    "            angle (Tensor): a Tensor of shape (4) where 4 is (iAtomId, jAtomId, kAtomId, lAtomId).\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): a Tensor of shape (N, M, 3) where N is the batch size, M is the number of atoms, 3 is the 3D coordinates (x, y, z).\n",
    "        \"\"\"\n",
    "        pos = []\n",
    "        confs = self.mol.GetConformers()\n",
    "        for conf in confs:\n",
    "            pos.append(torch.tensor(conf.GetPositions(),\n",
    "                                    dtype=torch.float32,\n",
    "                                    device=DEVICE))\n",
    "        pos = torch.stack(pos)\n",
    "        rIJ = pos[:, angle[1], :] - pos[:, angle[0], :]\n",
    "        rJK = pos[:, angle[2], :] - pos[:, angle[1], :]\n",
    "        rKL = pos[:, angle[3], :] - pos[:, angle[2], :]\n",
    "        nIJK = rIJ.cross(rJK, dim=2)\n",
    "        nJKL = rJK.cross(rKL, dim=2)\n",
    "        m = nIJK.cross(rJK)\n",
    "        N, _ = input.shape\n",
    "        values = input + torch.atan2(m.reshape(N, 1, 3).bmm(nJKL.reshape(N, 3, 1)).reshape(N))\n",
    "        rotAxisBegin = pos[:, angle[1], :]\n",
    "        rotAxisEnd = pos[:, angle[2], :]\n",
    "        rotAxis = rotAxisEnd - rotAxisBegin\n",
    "        rotAxis.norm(dim=1)\n",
    "        for it in self.alist[(angle[1], angle[2])]:\n",
    "            pos[:, it, :] -= rotAxisBegin\n",
    "            self.transformPoint(pos[:, it, :], values, rotAxis)\n",
    "            pos[:, it, :] += rotAxisBegin\n",
    "        return pos\n",
    "\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        An implementation of differentiable setDihedralRad from rdkit.\n",
    "        TODO: This version has eliminated all fault checks temporarily. Add them if needed from the link below.\n",
    "        See https://github.com/rdkit/rdkit/blob/master/Code/GraphMol/MolTransforms/MolTransforms.cpp#L612\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): a Tensor of shape (N, K) where N is the batch size, K is the number of dihedral angles for a conformer, 1 is (dihedral angle value).\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): a Tensor of shape (N, M, 3) where N is the batch size, M is the number of atoms, 3 is the 3D coordinates (x, y, z).\n",
    "        \"\"\"\n",
    "        N, K = input.shape\n",
    "        for i in range(K):\n",
    "            output = self.setDihedralRad(input[:, i], self.angles[i, :])\n",
    "        confs = self.mol.GetConformers()\n",
    "        for i in range(N):\n",
    "            for j in range(K):\n",
    "                Chem.rdMolTransforms.SetDihedralDeg(confs[i], \n",
    "                                                    self.angles[j, 0].item(),\n",
    "                                                    self.angles[j, 1].item(),\n",
    "                                                    self.angles[j, 2].item(),\n",
    "                                                    self.angles[j, 3].item(),\n",
    "                                                    input[i, j].item())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Write energy layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit.Chem.AllChem as Chem2\n",
    "\n",
    "\n",
    "class Energy(Module):\n",
    "    \"\"\"Energy loss with forward and backward pass.\"\"\"\n",
    "    \n",
    "    def __init__(self, mol: Mol):\n",
    "        self.mol = mol\n",
    "        self.ff_list = []\n",
    "\n",
    "    def forward(self, input: Tensor):\n",
    "        \"\"\"Forward pass of energy. Only mol matters here.\"\"\"\n",
    "        Chem2.MMFFSanitizeMolecule(self.mol)\n",
    "        mmff_props = Chem2.MMFFGetMoleculeProperties(self.mol)\n",
    "        energy = torch.tensor(0)\n",
    "        for i in range(self.mol.GetNumConformers()):\n",
    "            ff = Chem2.MMFFGetMoleculeForceField(self.mol, mmff_props, confId=i)\n",
    "            self.ff_list.append(ff)\n",
    "            energy += ff.CalcEnergy()\n",
    "        energy = energy / self.mol.GetNumConformers()\n",
    "        return energy\n",
    "\n",
    "\n",
    "    def backward(self, input: Tensor):\n",
    "        \"\"\"Backward pass of energy. Only ff_list from forward matters here.\"\"\"\n",
    "        grad_list = []\n",
    "        for ff in self.ff_list:\n",
    "            grad_list.append(torch.tensor(ff.CalcGrad()).reshape(1, -1, 3))\n",
    "        grad_energy = torch.stack(grad_list)\n",
    "        return grad_energy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Write base layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.distributions import StandardNormal\n",
    "from rdkit.Chem import AllChem as Chem2\n",
    "from rdkit.Chem import TorsionFingerprints\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def generate_branched_alkane(num_atoms: int) -> Chem.Mol:\n",
    "    \"\"\"Generates a branched alkane.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_atoms : int\n",
    "        Number of atoms in molecule to be generated.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles('CCCC')\n",
    "    edit_mol = Chem.RWMol(mol)\n",
    "    while edit_mol.GetNumAtoms() < num_atoms:\n",
    "        x = Chem.rdchem.Atom(6)\n",
    "        randidx = np.random.randint(len(edit_mol.GetAtoms()))\n",
    "        atom = edit_mol.GetAtomWithIdx(randidx)\n",
    "        if atom.GetDegree() > 2:\n",
    "            continue\n",
    "        if atom.GetDegree() == 2 and random.random() <= 0.5:\n",
    "            continue\n",
    "        idx = edit_mol.AddAtom(x)\n",
    "        edit_mol.AddBond(idx, randidx, Chem.rdchem.BondType.SINGLE)\n",
    "\n",
    "    Chem.SanitizeMol(edit_mol)\n",
    "    mol = Chem.rdmolops.AddHs(edit_mol.GetMol())\n",
    "\n",
    "    return mol\n",
    "\n",
    "\n",
    "def get_torsion_tuples(mol):\n",
    "    \"\"\"Gets the tuples for the torsion angles of the molecule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mol : RDKit molecule\n",
    "        Molecule for which torsion angles are to be extracted\n",
    "\n",
    "    * tuples_original, tuples_reindexed : list[int]\n",
    "        Tuples (quadruples) of indices that correspond to torsion angles. The first returns indices\n",
    "        for the original molecule and the second for a version of the molecule with Hydrogens removed\n",
    "        (since there are many cases where this stripped molecule is of interest)\n",
    "    \"\"\"\n",
    "\n",
    "    [mol.GetAtomWithIdx(i).SetProp(\"original_index\", str(i))\n",
    "     for i in range(mol.GetNumAtoms())]\n",
    "    stripped_mol = Chem2.rdmolops.RemoveHs(mol)\n",
    "\n",
    "    nonring, _ = TorsionFingerprints.CalculateTorsionLists(mol)\n",
    "    nonring_original = [list(atoms[0]) for atoms, ang in nonring]\n",
    "\n",
    "    original_to_stripped = {\n",
    "        int(stripped_mol.GetAtomWithIdx(reindex).GetProp(\"original_index\")): reindex\n",
    "        for reindex in range(stripped_mol.GetNumAtoms())\n",
    "    }\n",
    "    nonring_reindexed = [\n",
    "        [original_to_stripped[original] for original in atom_group]\n",
    "        for atom_group in nonring_original\n",
    "    ]\n",
    "\n",
    "    return nonring_original, nonring_reindexed\n",
    "\n",
    "\n",
    "class Base(StandardNormal):\n",
    "    \"\"\"Conformer initialization.\"\"\"\n",
    "\n",
    "    def __init__(self, num_atoms: int, batch_size: int):\n",
    "        \"\"\"Initialization of base distribution.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): an integer of batch size.\n",
    "            num_atoms (int): an integer of number of atoms.\n",
    "        \"\"\"\n",
    "        self.mol = generate_branched_alkane(num_atoms)\n",
    "        Chem.AllChem.EmbedMultipleConfs(self.mol, numConfs=batch_size)\n",
    "        Chem.rdForceFieldHelpers.MMFFOptimizeMoleculeConfs(self.mol, nonBondedThresh=10., )\n",
    "        self.torsion_angles, _ = get_torsion_tuples(self.mol)\n",
    "        self.torsion_angles = torch.tensor(self.torsion_angles)\n",
    "        super.__init__(shape=self.torsion_angles.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Combine flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nflows.flows import Flow\n",
    "from torch.nn import Sequential\n",
    "def make_model(num_atoms: int,\n",
    "               lower: float,\n",
    "               upper: float,\n",
    "               bijector: Mapping[str, Any],\n",
    "               base: Mapping[str, Any], \n",
    "               coord_trans: Mapping[str, Any],\n",
    "               energy_layer: Mapping[str, Any]\n",
    "               ) -> Tuple[Flow, Sequential]:\n",
    "  \"\"\"Constructs a particle model, with various configuration options.\n",
    "\n",
    "  With N particles, the model is implemented as follows:\n",
    "  1. We draw N particles randomly from a base distribution.\n",
    "  2. We jointly transform the particles with a flow (a Distrax bijector).\n",
    "\n",
    "  Optionally, the model can be made invariant to translations. We do this as\n",
    "  follows:\n",
    "  1. We draw N-1 particles and transform them with the flow as above.\n",
    "  2. We add an extra particle at a fixed location.\n",
    "  3. We choose a translation uniformly at random and apply it to all particles.\n",
    "\n",
    "  Args:\n",
    "    num_particles: number of particles.\n",
    "    lower: array of shape [dim], the lower ranges of the box.\n",
    "    upper: array of shape [dim], the upper ranges of the box.\n",
    "    bijector: configures the bijector that transforms particles. Expected to\n",
    "      have the following keys:\n",
    "      * 'constructor': a callable that creates the bijector.\n",
    "      * 'kwargs': keyword arguments to pass to the constructor.\n",
    "    base: configures the base distribution. Expected to have the following keys:\n",
    "      * 'constructor': a callable that creates the base distribution.\n",
    "      * 'kwargs': keyword arguments to pass to the constructor.\n",
    "\n",
    "  Returns:\n",
    "    A particle model.\n",
    "  \"\"\"\n",
    "  base_model = base['constructor'](\n",
    "      num_particles=num_atoms,\n",
    "      **base['kwargs'])\n",
    "  bij = bijector['constructor'](\n",
    "      angles=base_model.torsion_angles,\n",
    "      lower=lower,\n",
    "      upper=upper,\n",
    "      **bijector['kwargs'])\n",
    "\n",
    "  model = Flow(bij, base_model)\n",
    "  \n",
    "  trans = coord_trans['constructor'](\n",
    "      mol=base_model.mol,\n",
    "      angles=base_model.torsion_angles)\n",
    "  energy = energy_layer['constructor'](\n",
    "      mol=base_model.mol)\n",
    "  \n",
    "  energy_fn = Sequential(trans, energy)\n",
    "  \n",
    "  return model, energy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Write training/testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import config_dict\n",
    "from torch.nn import Transformer\n",
    "FREQUENCIES = {\n",
    "    4: 8,\n",
    "    8: 8,\n",
    "    16: 8,\n",
    "    32: 8,\n",
    "}\n",
    "\n",
    "def get_config(num_atoms: int):\n",
    "  \"\"\"Returns the config.\"\"\"\n",
    "  num_frequencies = FREQUENCIES[num_atoms]\n",
    "  train_batch_size = 128\n",
    "  config = config_dict.ConfigDict()\n",
    "  config.state = dict(\n",
    "      num_atoms=num_atoms,\n",
    "      beta=0.5,\n",
    "      lower=-torch.pi,\n",
    "      upper=torch.pi,\n",
    "  )\n",
    "  conditioner = dict(\n",
    "      constructor=make_equivariant_conditioner,\n",
    "      kwargs=dict(\n",
    "          embedding_size=256,\n",
    "          num_frequencies=num_frequencies,\n",
    "          conditioner_constructor=Transformer,\n",
    "          conditioner_kwargs=dict(\n",
    "              nhead=2,\n",
    "              num_encoder_layers=2,\n",
    "              num_decoder_layers=2,\n",
    "              dropout=0.,))\n",
    "  )\n",
    "  config.model = dict(\n",
    "      constructor=make_model,\n",
    "      kwargs=dict(\n",
    "          bijector=dict(\n",
    "              constructor=make_split_coupling_flow,\n",
    "              kwargs=dict(\n",
    "                  num_layers=24,\n",
    "                  num_bins=16,\n",
    "                  conditioner=conditioner,\n",
    "                  use_circular_shift=True,\n",
    "              ),\n",
    "          ),\n",
    "          base=dict(\n",
    "              constructor=Base,\n",
    "              kwargs=dict(\n",
    "                  num_atoms=num_atoms,\n",
    "                  batch_size=train_batch_size,\n",
    "              ),\n",
    "          ),\n",
    "          coord_trans=dict(\n",
    "              constructor=Dihedral2Coord,\n",
    "          ),\n",
    "          energy_layer=dict(\n",
    "              constructor=Energy,\n",
    "          ),\n",
    "      ),\n",
    "  )\n",
    "  config.train = dict(\n",
    "      batch_size=train_batch_size,\n",
    "      learning_rate=7e-5,\n",
    "      learning_rate_decay_steps=[250000, 500000],\n",
    "      learning_rate_decay_factor=0.1,\n",
    "      seed=42,\n",
    "      max_gradient_norm=10000.,\n",
    "  )\n",
    "  config.test = dict(\n",
    "      test_every=500,\n",
    "      batch_size=2048,\n",
    "  )\n",
    "  return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#\n",
    "# Copyright 2022 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Energy-based training of a flow model on an atomistic system.\"\"\"\n",
    "\n",
    "from typing import Callable, Dict, Tuple, Union\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "flags.DEFINE_enum('system', '_16',\n",
    "                  ['_4', '_8', '_32', '_64'\n",
    "                  ], 'System and number of atoms to train.')\n",
    "flags.DEFINE_integer('num_iterations', int(10**6), 'Number of training steps.')\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def _num_particles(system: str) -> int:\n",
    "  return int(system.split('_')[-1])\n",
    "\n",
    "\n",
    "def _get_loss(\n",
    "    model: Flow,\n",
    "    energy_fn: Callable,\n",
    "    beta: Tensor,\n",
    "    num_samples: int) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
    "  \"\"\"Returns the loss and stats.\"\"\"\n",
    "  samples, log_prob = model.sample_and_log_prob(\n",
    "      num_samples=num_samples)\n",
    "  energies = energy_fn(samples)\n",
    "  energy_loss = torch.mean(beta * energies + log_prob)\n",
    "\n",
    "  loss = energy_loss\n",
    "  stats = {\n",
    "      'energy': energies,\n",
    "      'model_log_prob': log_prob,\n",
    "      'target_log_prob': -beta * energies\n",
    "  }\n",
    "  return loss, stats\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  system = FLAGS.system\n",
    "  config = get_config(_num_particles(system))\n",
    "  # if system.startswith('lj'):\n",
    "  #   config = lennard_jones_config.get_config(_num_particles(system))\n",
    "  # elif system.startswith('mw_cubic'):\n",
    "  #   config = monatomic_water_config.get_config(_num_particles(system), 'cubic')\n",
    "  # elif system.startswith('mw_hex'):\n",
    "  #   config = monatomic_water_config.get_config(_num_particles(system), 'hex')\n",
    "  # else:\n",
    "  #   raise KeyError(system)\n",
    "\n",
    "  state = config.state\n",
    "  \n",
    "\n",
    "  def create_model():\n",
    "    return config.model['constructor'](\n",
    "        num_particles=state.num_particles,\n",
    "        lower=state.lower,\n",
    "        upper=state.upper,\n",
    "        **config.model['kwargs'])\n",
    "\n",
    "  def loss_fn():\n",
    "    \"\"\"Loss function for training.\"\"\"\n",
    "    model = create_model()\n",
    "\n",
    "    loss, stats = _get_loss(\n",
    "        model=model,\n",
    "        energy_fn=energy_fn_train,\n",
    "        beta=state.beta,\n",
    "        num_samples=config.train.batch_size,\n",
    "        )\n",
    "\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'energy': jnp.mean(stats['energy']),\n",
    "        'model_entropy': -jnp.mean(stats['model_log_prob']),\n",
    "    }\n",
    "    return loss, metrics\n",
    "\n",
    "  def eval_fn():\n",
    "    \"\"\"Evaluation function.\"\"\"\n",
    "    model = create_model()\n",
    "    loss, stats = _get_loss(\n",
    "        model=model,\n",
    "        energy_fn=energy_fn_test,\n",
    "        beta=state.beta,\n",
    "        num_samples=config.test.batch_size,\n",
    "        )\n",
    "    log_probs = {\n",
    "        'model_log_probs': stats['model_log_prob'],\n",
    "        'target_log_probs': stats['target_log_prob'],\n",
    "    }\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'energy': jnp.mean(stats['energy']),\n",
    "        'model_entropy': -jnp.mean(stats['model_log_prob']),\n",
    "        'ess': obs_utils.compute_ess(**log_probs),\n",
    "        'logz': obs_utils.compute_logz(**log_probs),\n",
    "        'logz_per_particle':\n",
    "            obs_utils.compute_logz(**log_probs) / state.num_particles,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "  print(f'Initialising system {system}')\n",
    "  rng_key = jax.random.PRNGKey(config.train.seed)\n",
    "  init_fn, apply_fn = hk.transform(loss_fn)\n",
    "  _, apply_eval_fn = hk.transform(eval_fn)\n",
    "\n",
    "  rng_key, init_key = jax.random.split(rng_key)\n",
    "  params = init_fn(init_key)\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  def _loss(params, rng):\n",
    "    loss, metrics = apply_fn(params, rng)\n",
    "    return loss, metrics\n",
    "  jitted_loss = jax.jit(jax.value_and_grad(_loss, has_aux=True))\n",
    "  jitted_eval = jax.jit(apply_eval_fn)\n",
    "\n",
    "  step = 0\n",
    "  print('Beginning of training.')\n",
    "  while step < FLAGS.num_iterations:\n",
    "    # Training update.\n",
    "    rng_key, loss_key = jax.random.split(rng_key)\n",
    "    (_, metrics), g = jitted_loss(params, loss_key)\n",
    "    if (step % 50) == 0:\n",
    "      print(f'Train[{step}]: {metrics}')\n",
    "    updates, opt_state = optimizer.update(g, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    if (step % config.test.test_every) == 0:\n",
    "      rng_key, val_key = jax.random.split(rng_key)\n",
    "      metrics = jitted_eval(params, val_key)\n",
    "      print(f'Valid[{step}]: {metrics}')\n",
    "\n",
    "    step += 1\n",
    "\n",
    "  print('Done')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  app.run(main)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "729135a7fed3b567d634efcd54e452b1f1e9c908d616bc775570d26a0a8b03e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
