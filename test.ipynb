{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models.base.Base & models.bijectors.CircularShift Test\n",
    "\n",
    "Problem: nflow generate samples whose require_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import make_model\n",
    "from experiments.configs import get_config\n",
    "config = get_config(16)\n",
    "model, energy_fn = make_model(\n",
    "    -torch.pi, torch.pi, **config.model['kwargs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transforms import Dihedral2Coord\n",
    "from models.energy import Energy\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "trans = Dihedral2Coord(\n",
    "    mol=model._distribution.mol,\n",
    "    angles=model._distribution.torsion_angles)\n",
    "    # mol=model._distribution.mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  0,  1,  2],\n",
       "        [ 0,  1,  2,  3],\n",
       "        [ 7,  4,  0,  5],\n",
       "        [ 8,  5,  0,  1],\n",
       "        [12,  7,  4, 14],\n",
       "        [11, 10,  5,  8],\n",
       "        [13, 11, 10,  5],\n",
       "        [15, 14,  4,  7]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._distribution.torsion_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_sample = torch.rand(size=(128, model._distribution.torsion_angles.shape[0]), requires_grad=True)\n",
    "coord_sample = trans(angles_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_sample = Energy.apply(coord_sample, model._distribution.mol)\n",
    "# coord_sample.register_hook(lambda grad: print(grad))\n",
    "loss = energy_sample.mean()\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-25.0157,   5.5373,   1.7646,  ...,  -9.2669,  -1.0254,  -0.1668],\n",
      "        [ -3.5710,  -1.8144,   2.1803,  ...,   3.2451,   4.0137, -19.7645],\n",
      "        [  1.0107,  -1.0144,   0.3451,  ...,  -2.6927,  -1.5392,  23.0678],\n",
      "        ...,\n",
      "        [ 23.6450,  25.6512,   0.2802,  ...,  11.0567,  -3.1502,  -6.1331],\n",
      "        [  2.5039,   9.6690,  -0.4413,  ...,   1.9024,   0.6822,  -1.0110],\n",
      "        [-17.2410,  13.7924,   1.1858,  ...,   6.0832,  -6.6854, -10.3659]])\n"
     ]
    }
   ],
   "source": [
    "print(angles_sample.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, logdet = model.sample_and_log_prob(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_loss = (loss + logdet[:, None]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0239, -0.3383,  0.3175,  ..., -0.1333, -0.3284, -0.4040],\n",
       "        [-0.1827,  1.1365, -0.5813,  ..., -0.3947,  0.3445,  0.7744],\n",
       "        [-0.0460,  0.5030, -0.5148,  ..., -0.1440,  0.2224,  0.9553],\n",
       "        ...,\n",
       "        [ 0.9095, -0.1586,  0.2059,  ...,  0.0287, -0.6542, -0.1489],\n",
       "        [-0.2702, -0.0838,  0.3339,  ..., -0.3490,  0.1917, -0.4530],\n",
       "        [-0.1773,  0.2773, -0.5189,  ..., -0.1973, -0.0033,  0.5414]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model._transform._transforms[0]._transforms[1].conditioner.linear1.weight.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 17])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = torch.rand(size=(4, 17))\n",
    "k = torch.rand(size=(4, 17))\n",
    "l = torch.rand(size=(4, 17))\n",
    "ls = [j, k, l]\n",
    "torch.stack(ls).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8167, 0.5797, 3.3655, 0.3703],\n",
       "         [0.7785, 2.3855, 0.4710, 2.0032],\n",
       "         [0.9357, 2.4022, 0.5809, 1.4372],\n",
       "         [0.9357, 2.4022, 0.4772, 0.6444],\n",
       "         [0.8134, 0.5167, 0.5041, 1.9168]]),\n",
       " tensor([[-2.0675, -1.5534,     nan, -0.5470],\n",
       "         [-2.0675,     nan, -1.2066,     nan],\n",
       "         [    nan,     nan, -0.1418,     nan],\n",
       "         [    nan,     nan, -1.2066, -1.1884],\n",
       "         [-2.0675, -1.5534, -0.1418,     nan]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "i=torch.rand(size=(5,4))\n",
    "j=torch.rand(size=(4, 17))\n",
    "k=torch.rand(size=(4, 17))\n",
    "l=torch.rand(size=(4, 17))\n",
    "from models.spline import _rational_quadratic_spline_fwd\n",
    "_rational_quadratic_spline_fwd(i,j,k,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "def _rqs_fwd_single(x: Tensor,\n",
    "                                   x_pos: Tensor,\n",
    "                                   y_pos: Tensor,\n",
    "                                   knot_slopes: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "  \"\"\"Applies a rational-quadratic spline to a scalar.\n",
    "  Args:\n",
    "    x: a scalar (0-dimensional array). The scalar `x` can be any real number; it\n",
    "      will be transformed by the spline if it's in the closed interval\n",
    "      `[x_pos[0], x_pos[-1]]`, and it will be transformed linearly if it's\n",
    "      outside that interval.\n",
    "    x_pos: array of shape [num_bins + 1], the bin boundaries on the x axis.\n",
    "    y_pos: array of shape [num_bins + 1], the bin boundaries on the y axis.\n",
    "    knot_slopes: array of shape [num_bins + 1], the slopes at the knot points.\n",
    "  Returns:\n",
    "    A tuple of two scalars: the output of the transformation and the log of the\n",
    "    absolute first derivative at `x`.\n",
    "  \"\"\"\n",
    "  # Search to find the right bin. NOTE: The bins are sorted, so we could use\n",
    "  # binary search, but this is more GPU/TPU friendly.\n",
    "  # The following implementation avoids indexing for faster TPU computation.\n",
    "  below_range = x <= x_pos[0]\n",
    "  above_range = x >= x_pos[-1]\n",
    "  correct_bin = torch.logical_and(x >= x_pos[:-1], x < x_pos[1:])\n",
    "  any_bin_in_range = torch.any(correct_bin)\n",
    "  first_bin = torch.concat([torch.tensor([1], dtype=bool),\n",
    "                               torch.zeros(len(correct_bin)-1, dtype=bool)])\n",
    "  # If y does not fall into any bin, we use the first spline in the following\n",
    "  # computations to avoid numerical issues.\n",
    "  correct_bin = torch.where(any_bin_in_range, correct_bin, first_bin)\n",
    "  # Dot product of each parameter with the correct bin mask.\n",
    "  params = torch.stack([x_pos, y_pos, knot_slopes], axis=1)\n",
    "  params_bin_left = torch.sum(correct_bin[:, None] * params[:-1], axis=0)\n",
    "  params_bin_right = torch.sum(correct_bin[:, None] * params[1:], axis=0)\n",
    "\n",
    "  x_pos_bin = (params_bin_left[0], params_bin_right[0])\n",
    "  y_pos_bin = (params_bin_left[1], params_bin_right[1])\n",
    "  knot_slopes_bin = (params_bin_left[2], params_bin_right[2])\n",
    "\n",
    "  bin_width = x_pos_bin[1] - x_pos_bin[0]\n",
    "  bin_height = y_pos_bin[1] - y_pos_bin[0]\n",
    "  bin_slope = bin_height / bin_width\n",
    "\n",
    "  z = (x - x_pos_bin[0]) / bin_width\n",
    "  # `z` should be in range [0, 1] to avoid NaNs later. This can happen because\n",
    "  # of small floating point issues or when x is outside of the range of bins.\n",
    "  # To avoid all problems, we restrict z in [0, 1].\n",
    "  z = torch.clip(z, 0., 1.)\n",
    "  sq_z = z * z\n",
    "  z1mz = z - sq_z  # z(1-z)\n",
    "  sq_1mz = (1. - z) ** 2\n",
    "  slopes_term = knot_slopes_bin[1] + knot_slopes_bin[0] - 2. * bin_slope\n",
    "  numerator = bin_height * (bin_slope * sq_z + knot_slopes_bin[0] * z1mz)\n",
    "  denominator = bin_slope + slopes_term * z1mz\n",
    "  y = y_pos_bin[0] + numerator / denominator\n",
    "\n",
    "  # Compute log det Jacobian.\n",
    "  # The logdet is a sum of 3 logs. It is easy to see that the inputs of the\n",
    "  # first two logs are guaranteed to be positive because we ensured that z is in\n",
    "  # [0, 1]. This is also true of the log(denominator) because:\n",
    "  # denominator\n",
    "  # == bin_slope + (knot_slopes_bin[1] + knot_slopes_bin[0] - 2 * bin_slope) *\n",
    "  # z*(1-z)\n",
    "  # >= bin_slope - 2 * bin_slope * z * (1-z)\n",
    "  # >= bin_slope - 2 * bin_slope * (1/4)\n",
    "  # == bin_slope / 2\n",
    "  logdet = 2. * torch.log(bin_slope) + torch.log(\n",
    "      knot_slopes_bin[1] * sq_z + 2. * bin_slope * z1mz +\n",
    "      knot_slopes_bin[0] * sq_1mz) - 2. * torch.log(denominator)\n",
    "\n",
    "  # If x is outside the spline range, we default to a linear transformation.\n",
    "  y = torch.where(below_range, (x - x_pos[0]) * knot_slopes[0] + y_pos[0], y)\n",
    "  y = torch.where(above_range, (x - x_pos[-1]) * knot_slopes[-1] + y_pos[-1], y)\n",
    "  logdet = torch.where(below_range, torch.log(knot_slopes[0]), logdet)\n",
    "  logdet = torch.where(above_range, torch.log(knot_slopes[-1]), logdet)\n",
    "  return y, logdet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "weight = torch.rand(size=(2,2), requires_grad=True)\n",
    "input = torch.rand(size=(1,2), requires_grad=True)\n",
    "output = LinearFunction.apply(input, weight)\n",
    "loss = output.mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(output.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "729135a7fed3b567d634efcd54e452b1f1e9c908d616bc775570d26a0a8b03e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
